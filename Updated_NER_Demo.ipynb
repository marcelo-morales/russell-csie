{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelo-morales/russell-csie/blob/main/Updated_NER_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktEzEcXEYJrS"
      },
      "source": [
        "# Spacy NER Demo\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AkTUzVleUot",
        "outputId": "f22b591d-c57b-4dfb-f33e-456f5546b8ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created blank 'en' model\n",
            "created ner\n"
          ]
        }
      ],
      "source": [
        "# # Load pre-existing spacy model\n",
        "import spacy\n",
        "# nlp=spacy.load('en_core_web_sm')\n",
        "\n",
        "# # Getting the pipeline component\n",
        "# ner=nlp.get_pipe(\"ner\")\n",
        "\n",
        "\n",
        "# if model is not None:\n",
        "#     nlp = spacy.load(model)  # load existing spacy model\n",
        "#     print(\"Loaded model '%s'\" % model)\n",
        "# else:\n",
        "nlp = spacy.blank('en')  # create blank Language class\n",
        "print(\"Created blank 'en' model\")\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    #ner = nlp.add_pipe('ner')\n",
        "    nlp.add_pipe(ner)\n",
        "    print(\"created ner\")\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNibT23PiPa4"
      },
      "source": [
        "#### Install Spacy if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmZHVOLfZKxl",
        "outputId": "23e54288-634c-4399-bebd-f681ad9810fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.15)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7mbayPNfO4o"
      },
      "source": [
        "### Example from tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNKbYXraelvB"
      },
      "outputs": [],
      "source": [
        "# # training data\n",
        "# TRAIN_DATA = [\n",
        "#               (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
        "#               (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
        "#               (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
        "#               (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
        "#               (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
        "#               (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
        "#               (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
        "#               (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "#               (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
        "#               (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
        "#               (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
        "#               (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
        "#               (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
        "#               (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "#               (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "#               (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
        "#               (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
        "#               (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
        "#               (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
        "#               ]\n",
        "\n",
        "# # Adding labels to the `ner`\n",
        "\n",
        "# for _, annotations in TRAIN_DATA:\n",
        "#   for ent in annotations.get(\"entities\"):\n",
        "#     ner.add_label(ent[2])\n",
        "\n",
        "# # Disable pipeline components you dont need to change\n",
        "# pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "# unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq-cIMFufRer"
      },
      "source": [
        "### Our Training Data - Defining the entity locations using char offsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Uyw7NldXfNBh"
      },
      "outputs": [],
      "source": [
        "# training data\n",
        "TRAIN_DATA = [\n",
        "              (\"what's for dinner\", {\"entities\": []}),\n",
        "              (\"got any dinner\", {\"entities\": []}),\n",
        "              (\"did you eat dinner\", {\"entities\": []}),\n",
        "              (\"wait what's going on again\", {\"entities\": []}),\n",
        "              (\"i'm confused\", {\"entities\": []}),\n",
        "              (\"where did you all learn frightened\", {\"entities\": []}),\n",
        "              (\"hi my name is\", {\"entities\": []}),\n",
        "              (\"what is this game called\", {\"entities\": []}),\n",
        "              (\"is it cold outside\", {\"entities\": []}),\n",
        "              (\"could you raise the thermostat\", {\"entities\": []}),\n",
        "              (\"call my nurse\", {\"entities\": []}),\n",
        "              (\"like that that's fair\", {\"entities\": []}),\n",
        "              (\"that's a butt\", {\"entities\": []}),\n",
        "              (\"it's a person\", {\"entities\": []}),\n",
        "              (\"oh yeah there's a persons\", {\"entities\": []}),\n",
        "              (\"yes does the character have a pleasant expression\", {\"entities\": []}),\n",
        "              # Note: take this null example out when beard trait is added\n",
        "              (\"does the character have a beard or a goatee\", {\"entities\": []}),\n",
        "              (\"no does a character have\", {\"entities\": []}),\n",
        "              # Notes: that this null example out when gender is added\n",
        "              (\"Is a character a male\", {\"entities\": []}),\n",
        "              (\"someone told me this would be fun\", {\"entities\": []}),\n",
        "              \n",
        "              (\"is your person wearing glasses\", {\"entities\": [(23,30,\"wearing_glasses\")]}),\n",
        "              (\"do they have glasses\", {\"entities\": [(13,20,\"wearing_glasses\")]}),\n",
        "              (\"does your person have glasses on\", {\"entities\": [(22,29,\"wearing_glasses\")]}),\n",
        "              (\"Does your person not have glasses on?\", {\"entities\": [(17,24,\"wearing_glasses\")]}),\n",
        "              (\"Does your character not have glasses on?\", {\"entities\": [(20,36,\"wearing_glasses\")]}),\n",
        "              (\"Does your character not wear glasses on?\", {\"entities\": [(20,36,\"wearing_glasses\")]}),\n",
        "              (\"Does she not have glasses on?\", {\"entities\": [(9,25,\"wearing_glasses\")]}),\n",
        "              (\"Does he not have glasses on?\", {\"entities\": [(8,24,\"wearing_glasses\")]}),\n",
        "              (\"Do they not wear glasses?\", {\"entities\": [(8,24,\"wearing_glasses\")]}),\n",
        "              (\"Does he not wear glasses?\", {\"entities\": [(8,24,\"wearing_glasses\")]}),\n",
        "\n",
        "              (\"is she blond\", {\"entities\": [(7,12,\"hair_color\")]}),\n",
        "              (\"is your person blond haired\", {\"entities\": [(15,20,\"hair_color\")]}),\n",
        "              (\"is he golden haired\", {\"entities\": [(6,12,\"hair_color\")]}),\n",
        "              (\"are they gold haired\", {\"entities\": [(9,13,\"hair_color\")]}),\n",
        "              (\"does your person have yellow hair\", {\"entities\": [(22,28,\"hair_color\")]}),\n",
        "              (\"are they auburn haired?\", {\"entities\": [(9,15,\"hair_color\")]}),\n",
        "              (\"is your person a ginger\", {\"entities\": [(17,24,\"hair_color\")]}),\n",
        "              (\"is their hair brown\", {\"entities\": [(14,19,\"hair_color\")]}),\n",
        "              (\"is her hair red\", {\"entities\": [(12,15,\"hair_color\")]}), \n",
        "              (\"is she red haired\", {\"entities\": [(7,10,\"hair_color\")]}),\n",
        "              (\"is she brunette\", {\"entities\": [(7,15,\"hair_color\")]}),\n",
        "              (\"is your character a brunette\" , {\"entities\": [(20,28,\"hair_color\")]}), \n",
        "              \n",
        "              (\"is your person wearing a red hat\", {\"entities\": [(24,27,\"hat_color\")]}),\n",
        "              (\"are they wearing a purple hat\", {\"entities\": [(19,25,\"hat_color\")]}),\n",
        "              (\"does your person have a yellow hat\", {\"entities\": [(24,30,\"hat_color\")]}), \n",
        "              (\"do they have a white hat\", {\"entities\": [(15,20,\"hat_color\")]}),\n",
        "\n",
        "              (\"does your person wear a hat\", {\"entities\": [(24,27,\"wearing_a_hat\")]}), \n",
        "              (\"do they wear a hat\", {\"entities\": [(15,18,\"wearing_a_hat\")]}), \n",
        "              (\"do they have a hat\", {\"entities\": [(15,18,\"wearing_a_hat\")]}),\n",
        "              (\"do they not have a hat?\", {\"entities\": [(8,22,\"wearing_a_hat\")]}),\n",
        "              (\"do they not wear a hat?\", {\"entities\": [(8,22,\"wearing_a_hat\")]}),\n",
        "              (\"does your character not have a hat?\", {\"entities\": [(19,33,\"wearing_a_hat\")]}),  \n",
        "              (\"does she not wear a hat?\", {\"entities\": [(9,23,\"wearing_a_hat\")]}), \n",
        "              (\"does your person not have a hat on?\", {\"entities\": [(17,31,\"wearing_a_hat\")]}),  \n",
        "\n",
        "              (\"does your person not have head hair\", {\"entities\": [(17,35,\"bald\")]}),\n",
        "              (\"is your person bald\", {\"entities\": [(15,19,\"bald\")]}),\n",
        "              (\"is she bald\", {\"entities\": [(7,13,\"bald\")]}),\n",
        "              (\"is he bald\", {\"entities\": [(6,10,\"bald\")]}),\n",
        "              (\"does he not have head hair\", {\"entities\": [(8,26,\"bald\")]}),\n",
        "              (\"does she not have hair on her head\", {\"entities\": [(8,21,\"bald\")]}),\n",
        "              (\"do they have hair on their head\", {\"entities\": [(8,17,\"bald\")]}),\n",
        "              (\"does she have hair\", {\"entities\": [(9,18,\"bald\")]}),\n",
        "              (\"is their head bald\", {\"entities\": [(14,18,\"bald\")]}),\n",
        "\n",
        "              (\"is your person bernard\", {\"entities\": [(15,22,\"name\")]}), \n",
        "              (\"is it anita\", {\"entities\": [(6,11,\"name\")]}),\n",
        "              (\"is she susan\", {\"entities\": [(7,12,\"name\")]}), \n",
        "              (\"is he sam\", {\"entities\": [(6,9,\"name\")]}), \n",
        "              (\"are they alex\", {\"entities\": [(9,13,\"name\")]}),\n",
        "              (\"are they paul\", {\"entities\": [(9,13,\"name\")]}),\n",
        "              (\"my guess is arthur\", {\"entities\": [(12,18,\"name\")]}),\n",
        "              (\"how about jamie \", {\"entities\": [(10,15,\"name\")]}),\n",
        "              (\"is your character charles\", {\"entities\": [(18,25,\"name\")]}),\n",
        "              (\"shannon\", {\"entities\": [(0,7,\"name\")]}), \n",
        "              (\"claire\", {\"entities\": [(0,6,\"name\")]})                                           \n",
        "              # (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]})\n",
        "              ]\n",
        "\n",
        "# Adding labels to the `ner`\n",
        "\n",
        "for _, annotations in TRAIN_DATA:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    ner.add_label(ent[2])\n",
        "    # print(\"adding label\")\n",
        "\n",
        "# Disable pipeline components you dont need to change\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL_mf98b6v0Y"
      },
      "source": [
        "### Training the model - Losses printed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXiwptMSev1x",
        "outputId": "576570ad-ed78-4dda-dd79-bb18487ea730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Losses {'ner': 1.5864062855586045}\n",
            "Losses {'ner': 3.3812718968053908}\n",
            "Losses {'ner': 4.928252290923226}\n",
            "Losses {'ner': 3.433539851916303}\n",
            "Losses {'ner': 7.490612097066881}\n",
            "Losses {'ner': 2.0163506172327628}\n",
            "Losses {'ner': 2.618139325040941}\n",
            "Losses {'ner': 0.8033159271994528}\n",
            "Losses {'ner': 3.2091078780515883}\n",
            "Losses {'ner': 0.11645594529980927}\n",
            "Losses {'ner': 3.697353937069508}\n",
            "Losses {'ner': 3.933366244549376}\n",
            "Losses {'ner': 0.5290001350545173}\n",
            "Losses {'ner': 0.6236849248418019}\n",
            "Losses {'ner': 7.497973076748637}\n",
            "Losses {'ner': 2.400886709943355}\n",
            "Losses {'ner': 4.641708965046803}\n",
            "Losses {'ner': 2.7683833990824516}\n",
            "Losses {'ner': 3.4245643334058284}\n",
            "Losses {'ner': 8.03824800827294}\n",
            "Losses {'ner': 0.13633653264131418}\n",
            "Losses {'ner': 0.3689143392809133}\n",
            "Losses {'ner': 0.11699374038474541}\n",
            "Losses {'ner': 4.591656780776343}\n",
            "Losses {'ner': 2.512691362109024}\n",
            "Losses {'ner': 0.8502826251445641}\n",
            "Losses {'ner': 0.40521948168129274}\n",
            "Losses {'ner': 2.615790376524825}\n",
            "Losses {'ner': 1.3238545345131116}\n",
            "Losses {'ner': 0.05454756603647226}\n",
            "Losses {'ner': 1.9178387655150024}\n",
            "Losses {'ner': 4.566804350106562}\n",
            "Losses {'ner': 1.6043621364478984}\n",
            "Losses {'ner': 0.27685282053784643}\n",
            "Losses {'ner': 0.11584527463233715}\n",
            "Losses {'ner': 0.837237612813624}\n",
            "Losses {'ner': 0.10957694780343102}\n",
            "Losses {'ner': 0.0070973907126285485}\n",
            "Losses {'ner': 1.8862127953804055}\n",
            "Losses {'ner': 0.9794135379560913}\n",
            "Losses {'ner': 1.2641730034318654}\n",
            "Losses {'ner': 0.1120475028394523}\n",
            "Losses {'ner': 0.08487133324695466}\n",
            "Losses {'ner': 0.01866660163010589}\n",
            "Losses {'ner': 1.180370106352222}\n",
            "Losses {'ner': 0.051909449636367476}\n",
            "Losses {'ner': 0.30651549244229637}\n",
            "Losses {'ner': 0.030692969328897465}\n",
            "Losses {'ner': 4.8561127060148745}\n",
            "Losses {'ner': 0.8000634837047433}\n",
            "Losses {'ner': 1.1706983746534336}\n",
            "Losses {'ner': 0.04896715173022165}\n",
            "Losses {'ner': 0.0036978270366624377}\n",
            "Losses {'ner': 0.002352346628526014}\n",
            "Losses {'ner': 0.1212128668164836}\n",
            "Losses {'ner': 1.2280837532996602}\n",
            "Losses {'ner': 0.0836974766327088}\n",
            "Losses {'ner': 0.006261980319095528}\n",
            "Losses {'ner': 0.09926231792901409}\n",
            "Losses {'ner': 0.014570639856574336}\n",
            "Losses {'ner': 0.004350797501394142}\n",
            "Losses {'ner': 0.6640099877510707}\n",
            "Losses {'ner': 0.00031869875399350247}\n",
            "Losses {'ner': 0.7503240825317793}\n",
            "Losses {'ner': 1.66447467342644}\n",
            "Losses {'ner': 4.2572965882341265}\n",
            "Losses {'ner': 0.5577061352793043}\n",
            "Losses {'ner': 3.2981036746301764}\n",
            "Losses {'ner': 2.19747874837001}\n",
            "Losses {'ner': 0.027102879177020478}\n",
            "Losses {'ner': 0.167631295976868}\n",
            "Losses {'ner': 1.8518923907321938}\n",
            "Losses {'ner': 2.7570642891208665}\n",
            "Losses {'ner': 0.35500218769256814}\n",
            "Losses {'ner': 0.004222749622758003}\n",
            "Losses {'ner': 0.9222055647258081}\n",
            "Losses {'ner': 0.11036353050941487}\n",
            "Losses {'ner': 9.578813158817036e-05}\n",
            "Losses {'ner': 2.832788630676817}\n",
            "Losses {'ner': 0.0033838520355321646}\n",
            "Losses {'ner': 0.44144729984552594}\n",
            "Losses {'ner': 0.06311827345761195}\n",
            "Losses {'ner': 0.04610884509131237}\n",
            "Losses {'ner': 3.5626561087991004}\n",
            "Losses {'ner': 1.4274447146051783}\n",
            "Losses {'ner': 0.002397769555603474}\n",
            "Losses {'ner': 0.016405103622738865}\n",
            "Losses {'ner': 0.5281785191615377}\n",
            "Losses {'ner': 1.6570351175506413}\n",
            "Losses {'ner': 0.00226802776800656}\n",
            "Losses {'ner': 0.27953144931366536}\n",
            "Losses {'ner': 0.394369939672917}\n",
            "Losses {'ner': 0.2826907528447657}\n",
            "Losses {'ner': 0.0004899683171660426}\n",
            "Losses {'ner': 0.0075144673329573955}\n",
            "Losses {'ner': 0.0038741693853736985}\n",
            "Losses {'ner': 0.0001162108125185459}\n",
            "Losses {'ner': 0.0699308713252724}\n",
            "Losses {'ner': 0.00017225949374417848}\n",
            "Losses {'ner': 0.00022459782647304331}\n",
            "Losses {'ner': 0.6993136943941063}\n",
            "Losses {'ner': 0.009027249297251886}\n",
            "Losses {'ner': 0.0789877139209757}\n",
            "Losses {'ner': 0.0012565453620189205}\n",
            "Losses {'ner': 0.10158682452810604}\n",
            "Losses {'ner': 1.3037640917424014}\n",
            "Losses {'ner': 0.008490160057449531}\n",
            "Losses {'ner': 0.02544987712176057}\n",
            "Losses {'ner': 1.6699603205957444e-05}\n",
            "Losses {'ner': 0.004569599532368096}\n",
            "Losses {'ner': 0.22336594650021574}\n",
            "Losses {'ner': 0.008075018122798178}\n",
            "Losses {'ner': 0.004178002308373186}\n",
            "Losses {'ner': 0.006679215658303256}\n",
            "Losses {'ner': 3.7459205488363945}\n",
            "Losses {'ner': 9.484337314930542}\n",
            "Losses {'ner': 0.03189699619198756}\n",
            "Losses {'ner': 0.027986786414431204}\n",
            "Losses {'ner': 0.29515416761713614}\n",
            "Losses {'ner': 0.00016692584785444467}\n",
            "Losses {'ner': 0.0033745906078028708}\n",
            "Losses {'ner': 0.3039729305095977}\n",
            "Losses {'ner': 0.5206504853293475}\n",
            "Losses {'ner': 0.0007329853881045289}\n",
            "Losses {'ner': 0.1548646331872644}\n",
            "Losses {'ner': 0.004438324218191638}\n",
            "Losses {'ner': 0.002050665527798437}\n",
            "Losses {'ner': 4.435307715195591e-05}\n",
            "Losses {'ner': 3.710707364979916e-05}\n",
            "Losses {'ner': 0.0011206586202490239}\n",
            "Losses {'ner': 0.26785154738558065}\n",
            "Losses {'ner': 0.001323073043206256}\n",
            "Losses {'ner': 1.2533621972556375}\n",
            "Losses {'ner': 0.7284207858732727}\n",
            "Losses {'ner': 0.0058926105384722845}\n",
            "Losses {'ner': 1.0751606507264106}\n",
            "Losses {'ner': 0.006446942588729429}\n",
            "Losses {'ner': 0.10044528560121821}\n",
            "Losses {'ner': 0.20517768215980428}\n",
            "Losses {'ner': 4.34231116917476}\n",
            "Losses {'ner': 4.263239227910171e-05}\n",
            "Losses {'ner': 0.0016840840907226418}\n",
            "Losses {'ner': 0.002971460653004173}\n",
            "Losses {'ner': 1.857326585697725}\n",
            "Losses {'ner': 0.8334323943018214}\n",
            "Losses {'ner': 1.6847113665834474}\n",
            "Losses {'ner': 0.026363967935374676}\n",
            "Losses {'ner': 1.3492452466722153}\n",
            "Losses {'ner': 0.0014236020738781416}\n",
            "Losses {'ner': 0.00027115012115596493}\n",
            "Losses {'ner': 3.7869997230675136}\n",
            "Losses {'ner': 0.12932507485658648}\n",
            "Losses {'ner': 0.032835084295143854}\n",
            "Losses {'ner': 0.013588865605512345}\n",
            "Losses {'ner': 5.0034713832027805e-05}\n",
            "Losses {'ner': 2.4093051977885294}\n",
            "Losses {'ner': 5.371045019686541e-06}\n",
            "Losses {'ner': 0.016454292666084943}\n",
            "Losses {'ner': 0.0007330675150922199}\n",
            "Losses {'ner': 0.03968350000090821}\n",
            "Losses {'ner': 3.561652477909159e-05}\n",
            "Losses {'ner': 1.8629879727390874}\n",
            "Losses {'ner': 1.8936937045754285}\n",
            "Losses {'ner': 0.3153853163990824}\n",
            "Losses {'ner': 0.0009362662257083959}\n",
            "Losses {'ner': 0.0003432895043303954}\n",
            "Losses {'ner': 0.010033359218361664}\n",
            "Losses {'ner': 0.016406205148243536}\n",
            "Losses {'ner': 0.028315692686560784}\n",
            "Losses {'ner': 0.0005084593328089465}\n",
            "Losses {'ner': 1.9480165845546175e-05}\n",
            "Losses {'ner': 0.0006294974599378813}\n",
            "Losses {'ner': 0.012745419021857107}\n",
            "Losses {'ner': 0.05107328032574076}\n",
            "Losses {'ner': 0.00026244201426660937}\n",
            "Losses {'ner': 0.7522483024862466}\n",
            "Losses {'ner': 0.003231940153899963}\n",
            "Losses {'ner': 4.6163402638348225}\n",
            "Losses {'ner': 1.8263738633378007e-06}\n",
            "Losses {'ner': 8.893396323838456e-05}\n",
            "Losses {'ner': 1.6046063455073127e-05}\n",
            "Losses {'ner': 0.00014828368915542538}\n",
            "Losses {'ner': 0.08723926587986741}\n",
            "Losses {'ner': 3.346443538266843e-05}\n",
            "Losses {'ner': 0.0011757843202824295}\n",
            "Losses {'ner': 0.8672261381078545}\n",
            "Losses {'ner': 0.28314026765745837}\n",
            "Losses {'ner': 0.0023887580340044975}\n",
            "Losses {'ner': 0.0014528371856846625}\n",
            "Losses {'ner': 2.4018451991429663}\n",
            "Losses {'ner': 0.0017770288611596407}\n",
            "Losses {'ner': 0.11066241024958201}\n",
            "Losses {'ner': 2.494834812180651e-05}\n",
            "Losses {'ner': 0.0008167090807021693}\n",
            "Losses {'ner': 0.0009372766344129402}\n",
            "Losses {'ner': 1.1633722936228683}\n",
            "Losses {'ner': 0.002154653450820627}\n",
            "Losses {'ner': 0.002058194238393329}\n",
            "Losses {'ner': 0.0005096452041829415}\n",
            "Losses {'ner': 2.807859238963775}\n"
          ]
        }
      ],
      "source": [
        "# Import requirements\n",
        "import random\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "# TRAINING THE MODEL\n",
        "with nlp.disable_pipes(*unaffected_pipes):\n",
        "\n",
        "  # Training for 30 iterations\n",
        "  for iteration in range(200): # train for longer, maybe 200 iterations or until loss < 0\n",
        "\n",
        "    # shuufling examples  before every iteration\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    losses = {}\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(TRAIN_DATA, size=64) # size=32 or 64 or even len(TRAIN_DATA) is better\n",
        "    for batch in batches:\n",
        "        texts, annotations = zip(*batch)\n",
        "        nlp.update(\n",
        "                    texts,  # batch of texts\n",
        "                    annotations,  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorize data\n",
        "                    losses=losses,\n",
        "                )\n",
        "    print(\"Losses\", losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI1FBYboyDI3"
      },
      "source": [
        "#### test code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMNL5v9PaWTy"
      },
      "outputs": [],
      "source": [
        "# # Import requirements\n",
        "# import random\n",
        "# from spacy.util import minibatch, compounding\n",
        "# from pathlib import Path\n",
        "\n",
        "# from spacy.training.example import Example\n",
        "\n",
        "# optimizer = nlp.begin_training()\n",
        "\n",
        "# # TRAINING THE MODEL\n",
        "# with nlp.disable_pipes(*unaffected_pipes):\n",
        "\n",
        "#   # Training for 30 iterations\n",
        "#   for iteration in range(30):\n",
        "\n",
        "#     # shuufling examples  before every iteration\n",
        "#     random.shuffle(TRAIN_DATA)\n",
        "#     losses = {}\n",
        "#     # batch up the examples using spaCy's minibatch\n",
        "#     batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "#     for batch in batches:\n",
        "#       for text, annotations in batch:\n",
        "#           # create Example\n",
        "#           doc = nlp.make_doc(text)\n",
        "#           example = Example.from_dict(doc, annotations)\n",
        "#           # Update the model\n",
        "#           nlp.update([example], losses=losses, drop=0.3)\n",
        "#         # from Spact 2.2.4, now on 3.0\n",
        "#         # nlp.update(\n",
        "#         #             texts,  # batch of texts\n",
        "#         #             annotations,  # batch of annotations\n",
        "#         #             drop=0.5,  # dropout - make it harder to memorise data\n",
        "#         #             losses=losses,\n",
        "#         #         )\n",
        "#     print(\"Losses\", losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO4l7xh5gLhj"
      },
      "source": [
        "### Tutorial Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "APSVq6jse27D",
        "outputId": "cfca982f-893c-47d0-de76-95d6d31b42ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entities [('Alto', 'PRODUCT')]\n",
            "Alto 16 20 PRODUCT\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I was driving a \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Alto\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# # Testing the model\n",
        "# doc = nlp(\"I was driving a Alto\")\n",
        "# print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "# from spacy import displacy\n",
        "\n",
        "# for ent in doc.ents:\n",
        "# \tprint(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "# displacy.render(doc, style='ent',jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_-AOA2ZgOlp"
      },
      "source": [
        "### Testing on audio transcription input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMnTdj7esuvV",
        "outputId": "8bdbde72-f556-4018-ce1b-dd0f3c171c62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: py: command not found\n"
          ]
        }
      ],
      "source": [
        "#pip install PyAudio-0.2.11-cp38-cp38-win_amd64.whl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWyoMF9ixRNL",
        "outputId": "cd7f7531-6c41-4203-e9d6-f373eead30f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libportaudio2 is already the newest version (19.6.0-1).\n",
            "libportaudiocpp0 is already the newest version (19.6.0-1).\n",
            "portaudio19-dev is already the newest version (19.6.0-1).\n",
            "libasound2-dev is already the newest version (1.1.3-5ubuntu0.6).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hTrVMK2rm9L",
        "outputId": "c12a959f-ac07-42a8-d943-f7228a358c28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyaudio in /usr/local/lib/python3.7/dist-packages (0.2.11)\n",
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.7/dist-packages (3.8.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyaudio\n",
        "!pip install SpeechRecognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwQpytSv5UdX"
      },
      "outputs": [],
      "source": [
        "## SPEECH TO TEXT TRANSCRIPTION CODE HERE\n",
        "import speech_recognition as sr\n",
        "import time \n",
        "#from gensim.parsing.preprocessing import remove_stopwords\n",
        "#look into finding a way to remove stop words without anaconda, installation issues\n",
        "#without using filtering out words library, i still dont catch umms and filler words, so \n",
        "#can just keep it like this?\n",
        "\n",
        "#using speech_recognition library\n",
        "#tutorial: https://realpython.com/python-speech-recognition/\n",
        "\n",
        "#input as a string\n",
        "#microphone low\n",
        "def recognize_speech(recognizer, microphone):\n",
        "    if not isinstance(recognizer, sr.Recognizer):\n",
        "        raise TypeError(\"`recognizer` must be `Recognizer` instance\")\n",
        "\n",
        "    if not isinstance(microphone, sr.Microphone):\n",
        "        raise TypeError(\"`microphone` must be `Microphone` instance\")\n",
        "\n",
        "    # adjust the recognizer sensitivity to ambient noise and record audio\n",
        "    # from the microphone\n",
        "    with microphone as source:\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "        audio = recognizer.listen(source)\n",
        "\n",
        "    # set up the response object\n",
        "    response = {\n",
        "        \"success\": True,\n",
        "        \"error\": None,\n",
        "        \"transcription\": None\n",
        "    }\n",
        "\n",
        "    # try recognizing the speech in the recording\n",
        "    # if a RequestError or UnknownValueError exception is caught,\n",
        "    #     update the response object accordingly\n",
        "    try:\n",
        "        response[\"transcription\"] = recognizer.recognize_google(audio)\n",
        "    except sr.RequestError:\n",
        "        # API was unreachable or unresponsive\n",
        "        response[\"success\"] = False\n",
        "        response[\"error\"] = \"API unavailable\"\n",
        "    except sr.UnknownValueError:\n",
        "        # speech was unintelligible\n",
        "        response[\"error\"] = \"Unable to recognize speech\"\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    recognizer = sr.Recognizer()\n",
        "    microphone = sr.Microphone()\n",
        "\n",
        "    print(\"these are the stopwords i will use \\n\")\n",
        "   # print(stopwords.words('english'))\n",
        "    #words_to_filter = set(stopwords.words('english'))\n",
        "    \n",
        "    \n",
        "\n",
        "    instruction = \"ask me question based on a specific attribute for my character\"\n",
        "    print(instruction)\n",
        "    time.sleep(1)\n",
        "\n",
        "    PROMPT_LIMIT = 1 #number of times a user is allowed to speak to microphone\n",
        "\n",
        "    for i in range(PROMPT_LIMIT):\n",
        "        response_from_user = recognize_speech(recognizer, microphone)\n",
        "        \n",
        "        if not response_from_user[\"success\"]:\n",
        "            break\n",
        "        print(\"I didn't catch that. What did you say?\\n\")\n",
        "\n",
        "    print(\"You said: {}\".format(response_from_user[\"transcription\"]))\n",
        "\n",
        "    # word_tokens = word_tokenize(response_from_user[\"transcription\"])\n",
        "\n",
        "    # filtered_sentence = [w for w in word_tokens if not w.lower() in words_to_filter]\n",
        " \n",
        "    # filtered_sentence = []\n",
        "\n",
        "    # for w in word_tokens:\n",
        "    #     if w not in words_to_filter:\n",
        "    #         filtered_sentence.append(w)\n",
        "\n",
        "    #print(\"after filtering out words we dont need, you said \" + str(filtered_sentence))\n",
        "\n",
        "    print(\"these are all the microphone inputs I can find \" + str(sr.Microphone.list_microphone_names()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndbh8j0x8mHC"
      },
      "source": [
        "## Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "DfNYmqxCgPvS",
        "outputId": "6847d43a-6eb8-4a87-fcde-92499e0a05cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entities [('red', 'hat_color')]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">is your character wearing \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    red\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">hat_color</span>\n",
              "</mark>\n",
              " hat</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "transcription = \"is your character wearing red hat\"\n",
        "\n",
        "# Testing the model\n",
        "doc = nlp(transcription)\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "# for ent in doc.ents:\n",
        "# \tprint(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "displacy.render(doc, style='ent',jupyter=True)\n",
        "\n",
        "## ent.text ('blond') and ent.label ( 'hair_color') will then be sent to the game backend to check\n",
        "##\n",
        "## guess_trait = ent.label\n",
        "## guess_adj = ent.text\n",
        "## if guess_trait == guess_adj:\n",
        "##\t\treturn affirmative_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "AhtNSupMfHoM",
        "outputId": "af3a4f02-f5ae-4b70-a22c-1036d8f6c91a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to /content\n",
            "Loading from /content\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-63823d5a9475>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the saved model and predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading from\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnlp_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp_updated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entities\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m def load(\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mfunc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mRegistryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spacy.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mlegacy_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spacy.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spacy-legacy.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E049] Can't find spaCy data directory: 'None'. Check your installation and permissions, or use spacy.util.set_data_path to customise the location if necessary."
          ]
        }
      ],
      "source": [
        "# Save the  model to directory\n",
        "output_dir = Path('/content/')\n",
        "nlp.to_disk(output_dir)\n",
        "print(\"Saved model to\", output_dir)\n",
        "\n",
        "transcription = \"is their hair brown?\" \n",
        "\n",
        "# Load the saved model and predict\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp_updated = spacy.load(output_dir)\n",
        "doc = nlp_updated(transcription)\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "jVy7n1_cDZe9",
        "outputId": "4c3703de-3c87-4c4a-f3cd-bccca12deebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading from /content\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7cce02d041e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the saved model and predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading from\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp_updated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entities\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m def load(\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mfunc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mRegistryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spacy.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mlegacy_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spacy.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spacy-legacy.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E049] Can't find spaCy data directory: 'None'. Check your installation and permissions, or use spacy.util.set_data_path to customise the location if necessary."
          ]
        }
      ],
      "source": [
        "# Load the saved model and predict\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp_updated = spacy.load(output_dir)\n",
        "doc = nlp_updated(transcription)\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "I7mbayPNfO4o",
        "OI1FBYboyDI3",
        "LO4l7xh5gLhj",
        "j_-AOA2ZgOlp"
      ],
      "include_colab_link": true,
      "name": "Updated_NER_Demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
