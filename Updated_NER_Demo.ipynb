{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Updated_NER_Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "I7mbayPNfO4o",
        "LO4l7xh5gLhj",
        "j_-AOA2ZgOlp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelo-morales/russell-csie/blob/main/Updated_NER_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy NER Demo\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ktEzEcXEYJrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load pre-existing spacy model\n",
        "import spacy\n",
        "# nlp=spacy.load('en_core_web_sm')\n",
        "\n",
        "# # Getting the pipeline component\n",
        "# ner=nlp.get_pipe(\"ner\")\n",
        "\n",
        "\n",
        "# if model is not None:\n",
        "#     nlp = spacy.load(model)  # load existing spacy model\n",
        "#     print(\"Loaded model '%s'\" % model)\n",
        "# else:\n",
        "nlp = spacy.blank('en')  # create blank Language class\n",
        "print(\"Created blank 'en' model\")\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    #ner = nlp.add_pipe('ner')\n",
        "    nlp.add_pipe(ner)\n",
        "    print(\"created ner\")\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')"
      ],
      "metadata": {
        "id": "5AkTUzVleUot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3aa416-515a-4375-ef0b-1b1757f5157b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created blank 'en' model\n",
            "created ner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Spacy if needed"
      ],
      "metadata": {
        "id": "pNibT23PiPa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U spacy"
      ],
      "metadata": {
        "id": "YmZHVOLfZKxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example from tutorial"
      ],
      "metadata": {
        "id": "I7mbayPNfO4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training data\n",
        "TRAIN_DATA = [\n",
        "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
        "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
        "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
        "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
        "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
        "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
        "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
        "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
        "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
        "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
        "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
        "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
        "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
        "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
        "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
        "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
        "              ]\n",
        "\n",
        "# Adding labels to the `ner`\n",
        "\n",
        "for _, annotations in TRAIN_DATA:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    ner.add_label(ent[2])\n",
        "\n",
        "# Disable pipeline components you dont need to change\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "metadata": {
        "id": "lNKbYXraelvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Our Training Data - Defining the entity locations using char offsets"
      ],
      "metadata": {
        "id": "Xq-cIMFufRer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training data\n",
        "TRAIN_DATA = [\n",
        "              (\"what's for dinner\", {\"entities\": []}),\n",
        "              (\"got any dinner\", {\"entities\": []}),\n",
        "              (\"did you eat dinner\", {\"entities\": []}),\n",
        "              (\"wait what's going on again\", {\"entities\": []}),\n",
        "              (\"i'm confused\", {\"entities\": []}),\n",
        "              (\"where did you all learn frightened\", {\"entities\": []}),\n",
        "              (\"hi my name is\", {\"entities\": []}),\n",
        "              (\"what is this game called\", {\"entities\": []}),\n",
        "              (\"is it cold outside\", {\"entities\": []}),\n",
        "              (\"could you raise the thermostat\", {\"entities\": []}),\n",
        "              (\"call my nurse\", {\"entities\": []}),\n",
        "              (\"like that that's fair\", {\"entities\": []}),\n",
        "              (\"that's a butt\", {\"entities\": []}),\n",
        "              (\"it's a person\", {\"entities\": []}),\n",
        "              (\"oh yeah there's a persons\", {\"entities\": []}),\n",
        "              (\"yes does the character have a pleasant expression\", {\"entities\": []}),\n",
        "              # Note: take this null example out when beard trait is added\n",
        "              (\"does the character have a beard or a goatee\", {\"entities\": []}),\n",
        "              (\"no does a character have\", {\"entities\": []}),\n",
        "              # Notes: that this null example out when gender is added\n",
        "              (\"Is a character a male\", {\"entities\": []}),\n",
        "              (\"someone told me this would be fun\", {\"entities\": []}),\n",
        "              \n",
        "              (\"is your person wearing glasses\", {\"entities\": [(23,30,\"wearing_glasses\")]}),\n",
        "              (\"do they have glasses\", {\"entities\": [(13,20,\"wearing_glasses\")]}),\n",
        "              (\"does your person have glasses on\", {\"entities\": [(22,29,\"wearing_glasses\")]}),\n",
        "              (\"Does your person not have glasses on?\", {\"entities\": [(17,24,\"wearing_glasses\")]}),\n",
        "              (\"Does your character not have glasses on?\", {\"entities\": [(20,36,\"wearing_glasses\")]}),\n",
        "              (\"Does your character not wear glasses on?\", {\"entities\": [(20,36,\"wearing_glasses\")]}),\n",
        "              (\"Does she not have glasses on?\", {\"entities\": [(9,25,\"wearing_glasses\")]}),\n",
        "              (\"Does he not have glasses on?\", {\"entities\": [(8,24,\"wearing_glasses\")]}),\n",
        "              (\"Do they not wear glasses?\", {\"entities\": [(8,24,\"wearing_glasses\")]}),\n",
        "              (\"Does he not wear glasses?\", {\"entities\": [(8,24,\"wearing_glasses\")]}),\n",
        "\n",
        "              (\"is she blond\", {\"entities\": [(7,12,\"hair_color\")]}),\n",
        "              (\"is your person blond haired\", {\"entities\": [(15,20,\"hair_color\")]}),\n",
        "              (\"is he golden haired\", {\"entities\": [(6,12,\"hair_color\")]}),\n",
        "              (\"are they gold haired\", {\"entities\": [(9,13,\"hair_color\")]}),\n",
        "              (\"does your person have yellow hair\", {\"entities\": [(22,28,\"hair_color\")]}),\n",
        "              (\"are they auburn haired?\", {\"entities\": [(9,15,\"hair_color\")]}),\n",
        "              (\"is your person a ginger\", {\"entities\": [(17,24,\"hair_color\")]}),\n",
        "              (\"is their hair brown\", {\"entities\": [(14,19,\"hair_color\")]}),\n",
        "              (\"is her hair red\", {\"entities\": [(12,15,\"hair_color\")]}), \n",
        "              (\"is she red haired\", {\"entities\": [(7,10,\"hair_color\")]}),             \n",
        "              \n",
        "              (\"is your person wearing a green hat\", {\"entities\": [(25,30,\"hat_color\")]}),\n",
        "              (\"are they wearing a green hat\", {\"entities\": [(19,24,\"hat_color\")]}),\n",
        "              (\"does your person have a green hat\", {\"entities\": [(24,29,\"hat_color\")]}), \n",
        "              (\"do they have a green hat\", {\"entities\": [(15,20,\"hat_color\")]}),\n",
        "\n",
        "              (\"does your person wear a hat\", {\"entities\": [(24,27,\"wearing_a_hat\")]}), \n",
        "              (\"do they wear a hat\", {\"entities\": [(15,18,\"wearing_a_hat\")]}), \n",
        "              (\"do they have a hat\", {\"entities\": [(15,18,\"wearing_a_hat\")]}),\n",
        "              (\"Do they not have a hat?\", {\"entities\": [(8,22,\"wearing_a_hat\")]}),\n",
        "              (\"Do they not wear a hat?\", {\"entities\": [(8,22,\"wearing_a_hat\")]}),  \n",
        "\n",
        "              (\"does your person not have head hair\", {\"entities\": [(17,35,\"bald\")]}),\n",
        "              (\"is your person bald\", {\"entities\": [(15,19,\"bald\")]}),\n",
        "              (\"is she bald\", {\"entities\": [(7,13,\"bald\")]}),\n",
        "              (\"is he bald\", {\"entities\": [(6,10,\"bald\")]}),\n",
        "              (\"does he not have head hair\", {\"entities\": [(8,26,\"bald\")]}),\n",
        "              (\"does she not have hair on her head\", {\"entities\": [(8,21,\"bald\")]}),\n",
        "              (\"do they have hair on their head\", {\"entities\": [(8,17,\"bald\")]}),\n",
        "              (\"does she have hair\", {\"entities\": [(9,18,\"bald\")]}),\n",
        "              (\"is their head bald\", {\"entities\": [(14,18,\"bald\")]}),\n",
        "\n",
        "              (\"is your person bernard\", {\"entities\": [(15,22,\"name\")]}), \n",
        "              (\"is it anita\", {\"entities\": [(6,11,\"name\")]}),\n",
        "              (\"is she susan\", {\"entities\": [(7,12,\"name\")]}), \n",
        "              (\"is he sam\", {\"entities\": [(6,9,\"name\")]}), \n",
        "              (\"are they alex\", {\"entities\": [(9,13,\"name\")]}),\n",
        "              (\"are they paul\", {\"entities\": [(9,13,\"name\")]}),\n",
        "              (\"my guess is arthur\", {\"entities\": [(12,18,\"name\")]}),\n",
        "              (\"how about jamie \", {\"entities\": [(10,15,\"name\")]}),\n",
        "              (\"is your character charles\", {\"entities\": [(18,25,\"name\")]}),\n",
        "              (\"shannon\", {\"entities\": [(0,7,\"name\")]}), \n",
        "              (\"claire\", {\"entities\": [(0,6,\"name\")]})                                           \n",
        "              # (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]})\n",
        "              ]\n",
        "\n",
        "# Adding labels to the `ner`\n",
        "\n",
        "for _, annotations in TRAIN_DATA:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    ner.add_label(ent[2])\n",
        "    # print(\"adding label\")\n",
        "\n",
        "# Disable pipeline components you dont need to change\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "metadata": {
        "id": "Uyw7NldXfNBh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model - Losses printed"
      ],
      "metadata": {
        "id": "tL_mf98b6v0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import requirements\n",
        "import random\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "# TRAINING THE MODEL\n",
        "with nlp.disable_pipes(*unaffected_pipes):\n",
        "\n",
        "  # Training for 30 iterations\n",
        "  for iteration in range(200): # train for longer, maybe 200 iterations or until loss < 0\n",
        "\n",
        "    # shuufling examples  before every iteration\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    losses = {}\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(TRAIN_DATA, size=64) # size=32 or 64 or even len(TRAIN_DATA) is better\n",
        "    for batch in batches:\n",
        "        texts, annotations = zip(*batch)\n",
        "        nlp.update(\n",
        "                    texts,  # batch of texts\n",
        "                    annotations,  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorise data\n",
        "                    losses=losses,\n",
        "                )\n",
        "    print(\"Losses\", losses)"
      ],
      "metadata": {
        "id": "DXiwptMSev1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### test code"
      ],
      "metadata": {
        "id": "OI1FBYboyDI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Import requirements\n",
        "# import random\n",
        "# from spacy.util import minibatch, compounding\n",
        "# from pathlib import Path\n",
        "\n",
        "# from spacy.training.example import Example\n",
        "\n",
        "# optimizer = nlp.begin_training()\n",
        "\n",
        "# # TRAINING THE MODEL\n",
        "# with nlp.disable_pipes(*unaffected_pipes):\n",
        "\n",
        "#   # Training for 30 iterations\n",
        "#   for iteration in range(30):\n",
        "\n",
        "#     # shuufling examples  before every iteration\n",
        "#     random.shuffle(TRAIN_DATA)\n",
        "#     losses = {}\n",
        "#     # batch up the examples using spaCy's minibatch\n",
        "#     batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "#     for batch in batches:\n",
        "#       for text, annotations in batch:\n",
        "#           # create Example\n",
        "#           doc = nlp.make_doc(text)\n",
        "#           example = Example.from_dict(doc, annotations)\n",
        "#           # Update the model\n",
        "#           nlp.update([example], losses=losses, drop=0.3)\n",
        "#         # from Spact 2.2.4, now on 3.0\n",
        "#         # nlp.update(\n",
        "#         #             texts,  # batch of texts\n",
        "#         #             annotations,  # batch of annotations\n",
        "#         #             drop=0.5,  # dropout - make it harder to memorise data\n",
        "#         #             losses=losses,\n",
        "#         #         )\n",
        "#     print(\"Losses\", losses)"
      ],
      "metadata": {
        "id": "gMNL5v9PaWTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tutorial Example"
      ],
      "metadata": {
        "id": "LO4l7xh5gLhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "doc = nlp(\"I was driving a Alto\")\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "for ent in doc.ents:\n",
        "\tprint(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "displacy.render(doc, style='ent',jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "APSVq6jse27D",
        "outputId": "cfca982f-893c-47d0-de76-95d6d31b42ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('Alto', 'PRODUCT')]\n",
            "Alto 16 20 PRODUCT\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I was driving a \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Alto\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6eRcKiZdi30W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on audio transcription input"
      ],
      "metadata": {
        "id": "j_-AOA2ZgOlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install PyAudio-0.2.11-cp38-cp38-win_amd64.whl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMnTdj7esuvV",
        "outputId": "8bdbde72-f556-4018-ce1b-dd0f3c171c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: py: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWyoMF9ixRNL",
        "outputId": "cd7f7531-6c41-4203-e9d6-f373eead30f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libportaudio2 is already the newest version (19.6.0-1).\n",
            "libportaudiocpp0 is already the newest version (19.6.0-1).\n",
            "portaudio19-dev is already the newest version (19.6.0-1).\n",
            "libasound2-dev is already the newest version (1.1.3-5ubuntu0.6).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyaudio\n",
        "!pip install SpeechRecognition\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hTrVMK2rm9L",
        "outputId": "c12a959f-ac07-42a8-d943-f7228a358c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyaudio in /usr/local/lib/python3.7/dist-packages (0.2.11)\n",
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.7/dist-packages (3.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SPEECH TO TEXT TRANSCRIPTION CODE HERE\n",
        "import speech_recognition as sr\n",
        "import time \n",
        "#from gensim.parsing.preprocessing import remove_stopwords\n",
        "#look into finding a way to remove stop words without anaconda, installation issues\n",
        "#without using filtering out words library, i still dont catch umms and filler words, so \n",
        "#can just keep it like this?\n",
        "\n",
        "#using speech_recognition library\n",
        "#tutorial: https://realpython.com/python-speech-recognition/\n",
        "\n",
        "#input as a string\n",
        "#microphone low\n",
        "def recognize_speech(recognizer, microphone):\n",
        "    if not isinstance(recognizer, sr.Recognizer):\n",
        "        raise TypeError(\"`recognizer` must be `Recognizer` instance\")\n",
        "\n",
        "    if not isinstance(microphone, sr.Microphone):\n",
        "        raise TypeError(\"`microphone` must be `Microphone` instance\")\n",
        "\n",
        "    # adjust the recognizer sensitivity to ambient noise and record audio\n",
        "    # from the microphone\n",
        "    with microphone as source:\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "        audio = recognizer.listen(source)\n",
        "\n",
        "    # set up the response object\n",
        "    response = {\n",
        "        \"success\": True,\n",
        "        \"error\": None,\n",
        "        \"transcription\": None\n",
        "    }\n",
        "\n",
        "    # try recognizing the speech in the recording\n",
        "    # if a RequestError or UnknownValueError exception is caught,\n",
        "    #     update the response object accordingly\n",
        "    try:\n",
        "        response[\"transcription\"] = recognizer.recognize_google(audio)\n",
        "    except sr.RequestError:\n",
        "        # API was unreachable or unresponsive\n",
        "        response[\"success\"] = False\n",
        "        response[\"error\"] = \"API unavailable\"\n",
        "    except sr.UnknownValueError:\n",
        "        # speech was unintelligible\n",
        "        response[\"error\"] = \"Unable to recognize speech\"\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    recognizer = sr.Recognizer()\n",
        "    microphone = sr.Microphone()\n",
        "\n",
        "    print(\"these are the stopwords i will use \\n\")\n",
        "   # print(stopwords.words('english'))\n",
        "    #words_to_filter = set(stopwords.words('english'))\n",
        "    \n",
        "    \n",
        "\n",
        "    instruction = \"ask me question based on a specific attribute for my character\"\n",
        "    print(instruction)\n",
        "    time.sleep(1)\n",
        "\n",
        "    PROMPT_LIMIT = 1 #number of times a user is allowed to speak to microphone\n",
        "\n",
        "    for i in range(PROMPT_LIMIT):\n",
        "        response_from_user = recognize_speech(recognizer, microphone)\n",
        "        \n",
        "        if not response_from_user[\"success\"]:\n",
        "            break\n",
        "        print(\"I didn't catch that. What did you say?\\n\")\n",
        "\n",
        "    print(\"You said: {}\".format(response_from_user[\"transcription\"]))\n",
        "\n",
        "    # word_tokens = word_tokenize(response_from_user[\"transcription\"])\n",
        "\n",
        "    # filtered_sentence = [w for w in word_tokens if not w.lower() in words_to_filter]\n",
        " \n",
        "    # filtered_sentence = []\n",
        "\n",
        "    # for w in word_tokens:\n",
        "    #     if w not in words_to_filter:\n",
        "    #         filtered_sentence.append(w)\n",
        "\n",
        "    #print(\"after filtering out words we dont need, you said \" + str(filtered_sentence))\n",
        "\n",
        "    print(\"these are all the microphone inputs I can find \" + str(sr.Microphone.list_microphone_names()))\n"
      ],
      "metadata": {
        "id": "fwQpytSv5UdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Model"
      ],
      "metadata": {
        "id": "Ndbh8j0x8mHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcription = \"do they not wear glasses\"\n",
        "\n",
        "# Testing the model\n",
        "doc = nlp(transcription)\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "# for ent in doc.ents:\n",
        "# \tprint(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "displacy.render(doc, style='ent',jupyter=True)\n",
        "\n",
        "## ent.text ('blond') and ent.label ( 'hair_color') will then be sent to the game backend to check\n",
        "##\n",
        "## guess_trait = ent.label\n",
        "## guess_adj = ent.text\n",
        "## if guess_trait == guess_adj:\n",
        "##\t\treturn affirmative_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "DfNYmqxCgPvS",
        "outputId": "2da34264-cac5-4f8d-c0c5-1942ba1b184f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('not wear glasses', 'wearing_glasses')]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">do they \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    not wear glasses\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">wearing_glasses</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the  model to directory\n",
        "output_dir = Path('/content/')\n",
        "nlp.to_disk(output_dir)\n",
        "print(\"Saved model to\", output_dir)\n",
        "\n",
        "transcription = \"is their hair brown?\" \n",
        "\n",
        "# Load the saved model and predict\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp_updated = spacy.load(output_dir)\n",
        "doc = nlp_updated(transcription)\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "id": "AhtNSupMfHoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b3ee32-5c62-4a37-fa94-3c9037426ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to /content\n",
            "Loading from /content\n",
            "Entities [('their', 'name'), ('brown', 'hair_color')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model and predict\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp_updated = spacy.load(output_dir)\n",
        "doc = nlp_updated(transcription)\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "id": "jVy7n1_cDZe9",
        "outputId": "e4f57c46-56ec-4189-c6e1-65efe417a8b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading from /content\n",
            "Entities [('their', 'name'), ('brown', 'hair_color')]\n"
          ]
        }
      ]
    }
  ]
}