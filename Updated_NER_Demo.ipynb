{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Updated_NER_Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "I7mbayPNfO4o",
        "LO4l7xh5gLhj",
        "j_-AOA2ZgOlp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelo-morales/russell-csie/blob/main/Updated_NER_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy NER Demo\n",
        "Caveats: Entity labels are not custom. We repurposing some of the predefined ones.\n",
        "\n",
        "Thus for our purposes:\n",
        "\n",
        "| Predefined Label  | Corresponds to our label \n",
        "| --- | --- |\n",
        "ORG | glasses\n",
        "DATE | hair_color\n",
        "NORP | hat_color\n",
        "GPE | hat\n",
        "LAW | bald\n",
        "\n",
        "Repurposing tutorial from: https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/\n",
        "\n"
      ],
      "metadata": {
        "id": "ktEzEcXEYJrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load pre-existing spacy model\n",
        "# import spacy\n",
        "# nlp=spacy.load('en_core_web_sm')\n",
        "\n",
        "# # Getting the pipeline component\n",
        "# ner=nlp.get_pipe(\"ner\")\n",
        "\n",
        "\n",
        "# if model is not None:\n",
        "#     nlp = spacy.load(model)  # load existing spacy model\n",
        "#     print(\"Loaded model '%s'\" % model)\n",
        "# else:\n",
        "nlp = spacy.blank('en')  # create blank Language class\n",
        "print(\"Created blank 'en' model\")\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    nlp.add_pipe(ner)\n",
        "    print(\"created ner\")\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')"
      ],
      "metadata": {
        "id": "5AkTUzVleUot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bcdb5a6-5161-4859-aa0a-b5d76a1d90bf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created blank 'en' model\n",
            "created ner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example from tutorial"
      ],
      "metadata": {
        "id": "I7mbayPNfO4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training data\n",
        "TRAIN_DATA = [\n",
        "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
        "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
        "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
        "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
        "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
        "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
        "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
        "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
        "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
        "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
        "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
        "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
        "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
        "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
        "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
        "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
        "              ]\n",
        "\n",
        "# Adding labels to the `ner`\n",
        "\n",
        "for _, annotations in TRAIN_DATA:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    ner.add_label(ent[2])\n",
        "\n",
        "# Disable pipeline components you dont need to change\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "metadata": {
        "id": "lNKbYXraelvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Our Training Data - Defining the entity locations using char offsets"
      ],
      "metadata": {
        "id": "Xq-cIMFufRer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training data\n",
        "## ORG - glasses\n",
        "## DATE - hair_color\n",
        "## NORP - hat_color\n",
        "## GPE = hat\n",
        "## LAW - bald\n",
        "TRAIN_DATA = [\n",
        "              (\"Is your person wearing glasses?\", {\"entities\": [(23,30,\"glasses\")]}),\n",
        "              (\"Do they have glasses?\", {\"entities\": [(13,20,\"glasses\")]}),\n",
        "              (\"Does your person have glasses on?\", {\"entities\": [(22,29,\"glasses\")]}),\n",
        "\n",
        "              (\"Is your person four-eyed?\", {\"entities\": [(15,24,\"glasses\")]}),\n",
        "              (\"Is she four-eyed?\", {\"entities\": [(7,16,\"glasses\")]}),\n",
        "\n",
        "              (\"Is she blond?\", {\"entities\": [(7,12,\"hair_color\")]}),\n",
        "              (\"Is your person blond-haired?\", {\"entities\": [(15,20,\"hair_color\")]}),\n",
        "              (\"Is he golden-haired?\", {\"entities\": [(6,12,\"hair_color\")]}),\n",
        "              (\"Are they gold-haired?\", {\"entities\": [(9,13,\"hair_color\")]}),\n",
        "              (\"Does your person have yellow hair?\", {\"entities\": [(22,28,\"hair_color\")]}),\n",
        "              (\"Are they auburn-haired?\", {\"entities\": [(9,15,\"hair_color\")]}),\n",
        "              (\"Is your person a ginger?\", {\"entities\": [(17,24,\"hair_color\")]}),              \n",
        "              \n",
        "              (\"Is your person wearing a green hat?\", {\"entities\": [(25,30,\"hat_color\")]}),\n",
        "              (\"Are they wearing a green hat?\", {\"entities\": [(19,24,\"hat_color\")]}),\n",
        "              (\"Does your person have a green hat?\", {\"entities\": [(24,29,\"hat_color\")]}), \n",
        "              (\"Do they have a green hat?\", {\"entities\": [(15,20,\"hat_color\")]}),\n",
        "\n",
        "              (\"Does your person wear a hat?\", {\"entities\": [(24,27,\"hat\")]}), \n",
        "              (\"Do they wear a hat?\", {\"entities\": [(15,18,\"hat\")]}), \n",
        "              (\"Do they have a hat?\", {\"entities\": [(15,18,\"hat\")]}), \n",
        "\n",
        "              (\"Does your person not have head hair?\", {\"entities\": [(17,35,\"bald\")]}),\n",
        "              (\"Is your person bald?\", {\"entities\": [(15,19,\"bald\")]}),\n",
        "              (\"Is she bald?\", {\"entities\": [(7,13,\"bald\")]}),\n",
        "              (\"Is he bald?\", {\"entities\": [(6,10,\"bald\")]}),\n",
        "              (\"Does he not have head hair?\", {\"entities\": [(8,26,\"bald\")]}),\n",
        "              (\"Does he not have hair on his head?\", {\"entities\": [(8,33,\"bald\")]})                                          \n",
        "              # (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]})\n",
        "              ]\n",
        "\n",
        "# Adding labels to the `ner`\n",
        "\n",
        "for _, annotations in TRAIN_DATA:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    ner.add_label(ent[2])\n",
        "    # print(\"adding label\")\n",
        "\n",
        "# Disable pipeline components you dont need to change\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "metadata": {
        "id": "Uyw7NldXfNBh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model - Losses printed"
      ],
      "metadata": {
        "id": "tL_mf98b6v0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import requirements\n",
        "import random\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "# TRAINING THE MODEL\n",
        "with nlp.disable_pipes(*unaffected_pipes):\n",
        "\n",
        "  # Training for 30 iterations\n",
        "  for iteration in range(30):\n",
        "\n",
        "    # shuufling examples  before every iteration\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    losses = {}\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "    for batch in batches:\n",
        "        texts, annotations = zip(*batch)\n",
        "        nlp.update(\n",
        "                    texts,  # batch of texts\n",
        "                    annotations,  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorise data\n",
        "                    losses=losses,\n",
        "                )\n",
        "    print(\"Losses\", losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXiwptMSev1x",
        "outputId": "4e21f61e-6521-457f-f4fb-28b3459dca35"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses {'ner': 116.55293642322567}\n",
            "Losses {'ner': 109.071293592453}\n",
            "Losses {'ner': 84.44517185614677}\n",
            "Losses {'ner': 84.12408847742176}\n",
            "Losses {'ner': 93.11449831812318}\n",
            "Losses {'ner': 94.15630834626356}\n",
            "Losses {'ner': 89.92942654829511}\n",
            "Losses {'ner': 98.1499682540962}\n",
            "Losses {'ner': 103.59412354729614}\n",
            "Losses {'ner': 96.90781155463628}\n",
            "Losses {'ner': 85.2396923679725}\n",
            "Losses {'ner': 83.30346892462383}\n",
            "Losses {'ner': 83.66889442287788}\n",
            "Losses {'ner': 96.42794564292058}\n",
            "Losses {'ner': 99.35342358176422}\n",
            "Losses {'ner': 85.59801035095006}\n",
            "Losses {'ner': 86.15292392495667}\n",
            "Losses {'ner': 89.57440308530158}\n",
            "Losses {'ner': 92.24757583745647}\n",
            "Losses {'ner': 94.03014397621155}\n",
            "Losses {'ner': 97.00736502019572}\n",
            "Losses {'ner': 81.8951233083732}\n",
            "Losses {'ner': 82.82368475520798}\n",
            "Losses {'ner': 81.91015132970522}\n",
            "Losses {'ner': 89.84439977835905}\n",
            "Losses {'ner': 83.46697818202665}\n",
            "Losses {'ner': 98.10295683896561}\n",
            "Losses {'ner': 87.71244052785372}\n",
            "Losses {'ner': 89.16375387860899}\n",
            "Losses {'ner': 88.25159345237762}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tutorial Example"
      ],
      "metadata": {
        "id": "LO4l7xh5gLhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "doc = nlp(\"I was driving a Alto\")\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "for ent in doc.ents:\n",
        "\tprint(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "displacy.render(doc, style='ent',jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "APSVq6jse27D",
        "outputId": "cfca982f-893c-47d0-de76-95d6d31b42ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('Alto', 'PRODUCT')]\n",
            "Alto 16 20 PRODUCT\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I was driving a \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Alto\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6eRcKiZdi30W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on audio transcription input"
      ],
      "metadata": {
        "id": "j_-AOA2ZgOlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install PyAudio-0.2.11-cp38-cp38-win_amd64.whl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMnTdj7esuvV",
        "outputId": "8bdbde72-f556-4018-ce1b-dd0f3c171c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: py: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWyoMF9ixRNL",
        "outputId": "cd7f7531-6c41-4203-e9d6-f373eead30f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libportaudio2 is already the newest version (19.6.0-1).\n",
            "libportaudiocpp0 is already the newest version (19.6.0-1).\n",
            "portaudio19-dev is already the newest version (19.6.0-1).\n",
            "libasound2-dev is already the newest version (1.1.3-5ubuntu0.6).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyaudio\n",
        "!pip install SpeechRecognition\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hTrVMK2rm9L",
        "outputId": "c12a959f-ac07-42a8-d943-f7228a358c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyaudio in /usr/local/lib/python3.7/dist-packages (0.2.11)\n",
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.7/dist-packages (3.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SPEECH TO TEXT TRANSCRIPTION CODE HERE\n",
        "import speech_recognition as sr\n",
        "import time \n",
        "#from gensim.parsing.preprocessing import remove_stopwords\n",
        "#look into finding a way to remove stop words without anaconda, installation issues\n",
        "#without using filtering out words library, i still dont catch umms and filler words, so \n",
        "#can just keep it like this?\n",
        "\n",
        "#using speech_recognition library\n",
        "#tutorial: https://realpython.com/python-speech-recognition/\n",
        "\n",
        "#input as a string\n",
        "#microphone low\n",
        "def recognize_speech(recognizer, microphone):\n",
        "    if not isinstance(recognizer, sr.Recognizer):\n",
        "        raise TypeError(\"`recognizer` must be `Recognizer` instance\")\n",
        "\n",
        "    if not isinstance(microphone, sr.Microphone):\n",
        "        raise TypeError(\"`microphone` must be `Microphone` instance\")\n",
        "\n",
        "    # adjust the recognizer sensitivity to ambient noise and record audio\n",
        "    # from the microphone\n",
        "    with microphone as source:\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "        audio = recognizer.listen(source)\n",
        "\n",
        "    # set up the response object\n",
        "    response = {\n",
        "        \"success\": True,\n",
        "        \"error\": None,\n",
        "        \"transcription\": None\n",
        "    }\n",
        "\n",
        "    # try recognizing the speech in the recording\n",
        "    # if a RequestError or UnknownValueError exception is caught,\n",
        "    #     update the response object accordingly\n",
        "    try:\n",
        "        response[\"transcription\"] = recognizer.recognize_google(audio)\n",
        "    except sr.RequestError:\n",
        "        # API was unreachable or unresponsive\n",
        "        response[\"success\"] = False\n",
        "        response[\"error\"] = \"API unavailable\"\n",
        "    except sr.UnknownValueError:\n",
        "        # speech was unintelligible\n",
        "        response[\"error\"] = \"Unable to recognize speech\"\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    recognizer = sr.Recognizer()\n",
        "    microphone = sr.Microphone()\n",
        "\n",
        "    print(\"these are the stopwords i will use \\n\")\n",
        "   # print(stopwords.words('english'))\n",
        "    #words_to_filter = set(stopwords.words('english'))\n",
        "    \n",
        "    \n",
        "\n",
        "    instruction = \"ask me question based on a specific attribute for my character\"\n",
        "    print(instruction)\n",
        "    time.sleep(1)\n",
        "\n",
        "    PROMPT_LIMIT = 1 #number of times a user is allowed to speak to microphone\n",
        "\n",
        "    for i in range(PROMPT_LIMIT):\n",
        "        response_from_user = recognize_speech(recognizer, microphone)\n",
        "        \n",
        "        if not response_from_user[\"success\"]:\n",
        "            break\n",
        "        print(\"I didn't catch that. What did you say?\\n\")\n",
        "\n",
        "    print(\"You said: {}\".format(response_from_user[\"transcription\"]))\n",
        "\n",
        "    # word_tokens = word_tokenize(response_from_user[\"transcription\"])\n",
        "\n",
        "    # filtered_sentence = [w for w in word_tokens if not w.lower() in words_to_filter]\n",
        " \n",
        "    # filtered_sentence = []\n",
        "\n",
        "    # for w in word_tokens:\n",
        "    #     if w not in words_to_filter:\n",
        "    #         filtered_sentence.append(w)\n",
        "\n",
        "    #print(\"after filtering out words we dont need, you said \" + str(filtered_sentence))\n",
        "\n",
        "    print(\"these are all the microphone inputs I can find \" + str(sr.Microphone.list_microphone_names()))\n"
      ],
      "metadata": {
        "id": "fwQpytSv5UdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Model"
      ],
      "metadata": {
        "id": "Ndbh8j0x8mHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcription = \"Do they have blond hair and red glasses?\"\n",
        "\n",
        "# Testing the model\n",
        "doc = nlp(transcription)\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "# for ent in doc.ents:\n",
        "# \tprint(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "displacy.render(doc, style='ent',jupyter=True)\n",
        "\n",
        "## ent.text ('blond') and ent.label ('DATE', which will ultimately be 'hair_color') will then be sent to the game backend to check\n",
        "##\n",
        "## guess_trait = ent.label\n",
        "## guess_adj = ent.text\n",
        "## if guess_trait == guess_adj:\n",
        "##\t\treturn affirmative_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "DfNYmqxCgPvS",
        "outputId": "1eeacbbd-daa1-47dd-d8a8-773a39b1ab4c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('blond', 'hair_color')]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Do they have \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    blond\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">hair_color</span>\n",
              "</mark>\n",
              " hair and red glasses?</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| Predefined Label  | Corresponds to our label \n",
        "| --- | --- |\n",
        "ORG | glasses\n",
        "DATE | hair_color\n",
        "NORP | hat_color\n",
        "GPE | hat\n",
        "LAW | bald"
      ],
      "metadata": {
        "id": "6KnuE35W7VKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the  model to directory\n",
        "output_dir = Path('/content/')\n",
        "nlp.to_disk(output_dir)\n",
        "print(\"Saved model to\", output_dir)\n",
        "\n",
        "# Load the saved model and predict\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp_updated = spacy.load(output_dir)\n",
        "doc = nlp_updated(\"Fridge can be ordered in FlipKart\" )\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "id": "AhtNSupMfHoM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}