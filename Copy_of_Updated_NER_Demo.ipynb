{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelo-morales/russell-csie/blob/main/Copy_of_Updated_NER_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktEzEcXEYJrS"
      },
      "source": [
        "# Spacy NER Demo\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AkTUzVleUot",
        "outputId": "fef96b9a-4f89-41db-a1ba-9b64d9819fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created blank 'en' model\n",
            "created ner\n"
          ]
        }
      ],
      "source": [
        "# # Load pre-existing spacy model\n",
        "import spacy\n",
        "# nlp=spacy.load('en_core_web_sm')\n",
        "\n",
        "# # Getting the pipeline component\n",
        "# ner=nlp.get_pipe(\"ner\")\n",
        "\n",
        "\n",
        "# if model is not None:\n",
        "#     nlp = spacy.load(model)  # load existing spacy model\n",
        "#     print(\"Loaded model '%s'\" % model)\n",
        "# else:\n",
        "nlp = spacy.blank('en')  # create blank Language class\n",
        "print(\"Created blank 'en' model\")\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    #ner = nlp.add_pipe('ner')\n",
        "    nlp.add_pipe(ner)\n",
        "    print(\"created ner\")\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmZHVOLfZKxl"
      },
      "outputs": [],
      "source": [
        "pip install -U spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7mbayPNfO4o"
      },
      "source": [
        "### Example from tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNKbYXraelvB"
      },
      "outputs": [],
      "source": [
        "# training data\n",
        "TRAIN_DATA = [\n",
        "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
        "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
        "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
        "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
        "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
        "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
        "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
        "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
        "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
        "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
        "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
        "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
        "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
        "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
        "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
        "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
        "              ]\n",
        "\n",
        "# Adding labels to the `ner`\n",
        "\n",
        "for _, annotations in TRAIN_DATA:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    ner.add_label(ent[2])\n",
        "\n",
        "# Disable pipeline components you dont need to change\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq-cIMFufRer"
      },
      "source": [
        "### Our Training Data - Defining the entity locations using char offsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uyw7NldXfNBh"
      },
      "outputs": [],
      "source": [
        "# training data\n",
        "TRAIN_DATA = [\n",
        "              (\"Is your person wearing glasses?\", {\"entities\": [(23,30,\"glasses\")]}),\n",
        "              (\"Do they have glasses?\", {\"entities\": [(13,20,\"glasses\")]}),\n",
        "              (\"Does your person have glasses on?\", {\"entities\": [(22,29,\"glasses\")]}),\n",
        "              (\"Does your person not have glasses on?\", {\"entities\": [(17,24,\"glasses\")]}),\n",
        "              (\"Does he not have glasses on?\", {\"entities\": [(8,24,\"glasses\")]}),\n",
        "              (\"Do they not wear glasses?\", {\"entities\": [(8,24,\"glasses\")]}),\n",
        "              (\"Does he not wear glasses?\", {\"entities\": [(8,24,\"glasses\")]}),\n",
        "\n",
        "              (\"Is your person four-eyed?\", {\"entities\": [(15,24,\"glasses\")]}),\n",
        "              (\"Is she four-eyed?\", {\"entities\": [(7,16,\"glasses\")]}),\n",
        "\n",
        "              (\"Is she blond?\", {\"entities\": [(7,12,\"hair_color\")]}),\n",
        "              (\"Is your person blond-haired?\", {\"entities\": [(15,20,\"hair_color\")]}),\n",
        "              (\"Is he golden-haired?\", {\"entities\": [(6,12,\"hair_color\")]}),\n",
        "              (\"Are they gold-haired?\", {\"entities\": [(9,13,\"hair_color\")]}),\n",
        "              (\"Does your person have yellow hair?\", {\"entities\": [(22,28,\"hair_color\")]}),\n",
        "              (\"Are they auburn-haired?\", {\"entities\": [(9,15,\"hair_color\")]}),\n",
        "              (\"Is your person a ginger?\", {\"entities\": [(17,24,\"hair_color\")]}),              \n",
        "              \n",
        "              (\"Is your person wearing a green hat?\", {\"entities\": [(25,30,\"hat_color\")]}),\n",
        "              (\"Are they wearing a green hat?\", {\"entities\": [(19,24,\"hat_color\")]}),\n",
        "              (\"Does your person have a green hat?\", {\"entities\": [(24,29,\"hat_color\")]}), \n",
        "              (\"Do they have a green hat?\", {\"entities\": [(15,20,\"hat_color\")]}),\n",
        "\n",
        "              (\"Does your person wear a hat?\", {\"entities\": [(24,27,\"hat\")]}), \n",
        "              (\"Do they wear a hat?\", {\"entities\": [(15,18,\"hat\")]}), \n",
        "              (\"Do they have a hat?\", {\"entities\": [(15,18,\"hat\")]}), \n",
        "\n",
        "              (\"Does your person not have head hair?\", {\"entities\": [(17,35,\"bald\")]}),\n",
        "              (\"Is your person bald?\", {\"entities\": [(15,19,\"bald\")]}),\n",
        "              (\"Is she bald?\", {\"entities\": [(7,13,\"bald\")]}),\n",
        "              (\"Is he bald?\", {\"entities\": [(6,10,\"bald\")]}),\n",
        "              (\"Does he not have head hair?\", {\"entities\": [(8,26,\"bald\")]}),\n",
        "              (\"Does he not have hair on his head?\", {\"entities\": [(8,33,\"bald\")]}),\n",
        "\n",
        "              (\"Is your person dave?\", {\"entities\": [(15,19,\"character_guess\")]}), \n",
        "              (\"Is it sarah?\", {\"entities\": [(6,11,\"character_guess\")]}),\n",
        "              (\"Is she kelly?\", {\"entities\": [(7,12,\"character_guess\")]}), \n",
        "              (\"Is he sam?\", {\"entities\": [(6,9,\"character_guess\")]}), \n",
        "              (\"Are they alex?\", {\"entities\": [(9,13,\"character_guess\")]}), \n",
        "              (\"Is your character harry?\", {\"entities\": [(18,23,\"character_guess\")]}),\n",
        "              (\"sarah?\", {\"entities\": [(0,5,\"character_guess\")]}), \n",
        "              (\"james?\", {\"entities\": [(0,5,\"character_guess\")]}),\n",
        "              (\"anita?\", {\"entities\": [(0,5,\"character_guess\")]}),\n",
        "              (\"claire?\", {\"entities\": [(0,5,\"character_guess\")]}),\n",
        "              (\"are they anita?\", {\"entities\": [(8,14,\"character_guess\")]}),\n",
        "              (\"anita?\", {\"entities\": [(0,5,\"character_guess\")]})                                                 \n",
        "              # (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]})\n",
        "              ]\n",
        "\n",
        "# Adding labels to the `ner`\n",
        "\n",
        "for _, annotations in TRAIN_DATA:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    ner.add_label(ent[2])\n",
        "    # print(\"adding label\")\n",
        "\n",
        "# Disable pipeline components you dont need to change\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL_mf98b6v0Y"
      },
      "source": [
        "### Training the model - Losses printed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXiwptMSev1x",
        "outputId": "a09a9750-8a03-4601-d786-4cd0f8d1aa30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses {'ner': 6.662823427792389}\n",
            "Losses {'ner': 1.7198142624682626}\n",
            "Losses {'ner': 4.348802664114407}\n",
            "Losses {'ner': 1.8888803463903983}\n",
            "Losses {'ner': 0.32423657590923716}\n",
            "Losses {'ner': 1.2962877064713172}\n",
            "Losses {'ner': 1.5783482554702672}\n",
            "Losses {'ner': 1.6724991051887865}\n",
            "Losses {'ner': 2.332113292693946}\n",
            "Losses {'ner': 0.9737775834920215}\n",
            "Losses {'ner': 1.3339262458558998}\n",
            "Losses {'ner': 1.3064564355842565}\n",
            "Losses {'ner': 0.003415596400663218}\n",
            "Losses {'ner': 0.1832488964401536}\n",
            "Losses {'ner': 0.012343694289127922}\n",
            "Losses {'ner': 3.681365966246798}\n",
            "Losses {'ner': 4.625484008952893}\n",
            "Losses {'ner': 0.2846256189198406}\n",
            "Losses {'ner': 1.6135093097850688}\n",
            "Losses {'ner': 0.05449728068289383}\n",
            "Losses {'ner': 1.987639040476154e-05}\n",
            "Losses {'ner': 3.92574622263264}\n",
            "Losses {'ner': 2.033725938598217}\n",
            "Losses {'ner': 0.0007907980716196672}\n",
            "Losses {'ner': 0.010385550046653142}\n",
            "Losses {'ner': 0.024632522102387405}\n",
            "Losses {'ner': 5.067517358414537}\n",
            "Losses {'ner': 0.0003986016458497365}\n",
            "Losses {'ner': 6.188629789823022}\n",
            "Losses {'ner': 0.5853581264131457}\n"
          ]
        }
      ],
      "source": [
        "# Import requirements\n",
        "import random\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "# TRAINING THE MODEL\n",
        "with nlp.disable_pipes(*unaffected_pipes):\n",
        "\n",
        "  # Training for 30 iterations\n",
        "  for iteration in range(30):\n",
        "\n",
        "    # shuufling examples  before every iteration\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    losses = {}\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "    for batch in batches:\n",
        "        texts, annotations = zip(*batch)\n",
        "        nlp.update(\n",
        "                    texts,  # batch of texts\n",
        "                    annotations,  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorise data\n",
        "                    losses=losses,\n",
        "                )\n",
        "    print(\"Losses\", losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI1FBYboyDI3"
      },
      "source": [
        "#### test code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMNL5v9PaWTy"
      },
      "outputs": [],
      "source": [
        "# # Import requirements\n",
        "# import random\n",
        "# from spacy.util import minibatch, compounding\n",
        "# from pathlib import Path\n",
        "\n",
        "# from spacy.training.example import Example\n",
        "\n",
        "# optimizer = nlp.begin_training()\n",
        "\n",
        "# # TRAINING THE MODEL\n",
        "# with nlp.disable_pipes(*unaffected_pipes):\n",
        "\n",
        "#   # Training for 30 iterations\n",
        "#   for iteration in range(30):\n",
        "\n",
        "#     # shuufling examples  before every iteration\n",
        "#     random.shuffle(TRAIN_DATA)\n",
        "#     losses = {}\n",
        "#     # batch up the examples using spaCy's minibatch\n",
        "#     batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "#     for batch in batches:\n",
        "#       for text, annotations in batch:\n",
        "#           # create Example\n",
        "#           doc = nlp.make_doc(text)\n",
        "#           example = Example.from_dict(doc, annotations)\n",
        "#           # Update the model\n",
        "#           nlp.update([example], losses=losses, drop=0.3)\n",
        "#         # from Spact 2.2.4, now on 3.0\n",
        "#         # nlp.update(\n",
        "#         #             texts,  # batch of texts\n",
        "#         #             annotations,  # batch of annotations\n",
        "#         #             drop=0.5,  # dropout - make it harder to memorise data\n",
        "#         #             losses=losses,\n",
        "#         #         )\n",
        "#     print(\"Losses\", losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO4l7xh5gLhj"
      },
      "source": [
        "### Tutorial Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "APSVq6jse27D",
        "outputId": "cfca982f-893c-47d0-de76-95d6d31b42ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entities [('Alto', 'PRODUCT')]\n",
            "Alto 16 20 PRODUCT\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I was driving a \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Alto\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Testing the model\n",
        "doc = nlp(\"I was driving a Alto\")\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "for ent in doc.ents:\n",
        "\tprint(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "displacy.render(doc, style='ent',jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eRcKiZdi30W"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_-AOA2ZgOlp"
      },
      "source": [
        "### Testing on audio transcription input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMnTdj7esuvV",
        "outputId": "8bdbde72-f556-4018-ce1b-dd0f3c171c62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: py: command not found\n"
          ]
        }
      ],
      "source": [
        "#pip install PyAudio-0.2.11-cp38-cp38-win_amd64.whl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWyoMF9ixRNL",
        "outputId": "cd7f7531-6c41-4203-e9d6-f373eead30f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libportaudio2 is already the newest version (19.6.0-1).\n",
            "libportaudiocpp0 is already the newest version (19.6.0-1).\n",
            "portaudio19-dev is already the newest version (19.6.0-1).\n",
            "libasound2-dev is already the newest version (1.1.3-5ubuntu0.6).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hTrVMK2rm9L",
        "outputId": "c12a959f-ac07-42a8-d943-f7228a358c28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyaudio in /usr/local/lib/python3.7/dist-packages (0.2.11)\n",
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.7/dist-packages (3.8.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyaudio\n",
        "!pip install SpeechRecognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwQpytSv5UdX"
      },
      "outputs": [],
      "source": [
        "## SPEECH TO TEXT TRANSCRIPTION CODE HERE\n",
        "import speech_recognition as sr\n",
        "import time \n",
        "#from gensim.parsing.preprocessing import remove_stopwords\n",
        "#look into finding a way to remove stop words without anaconda, installation issues\n",
        "#without using filtering out words library, i still dont catch umms and filler words, so \n",
        "#can just keep it like this?\n",
        "\n",
        "#using speech_recognition library\n",
        "#tutorial: https://realpython.com/python-speech-recognition/\n",
        "\n",
        "#input as a string\n",
        "#microphone low\n",
        "def recognize_speech(recognizer, microphone):\n",
        "    if not isinstance(recognizer, sr.Recognizer):\n",
        "        raise TypeError(\"`recognizer` must be `Recognizer` instance\")\n",
        "\n",
        "    if not isinstance(microphone, sr.Microphone):\n",
        "        raise TypeError(\"`microphone` must be `Microphone` instance\")\n",
        "\n",
        "    # adjust the recognizer sensitivity to ambient noise and record audio\n",
        "    # from the microphone\n",
        "    with microphone as source:\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "        audio = recognizer.listen(source)\n",
        "\n",
        "    # set up the response object\n",
        "    response = {\n",
        "        \"success\": True,\n",
        "        \"error\": None,\n",
        "        \"transcription\": None\n",
        "    }\n",
        "\n",
        "    # try recognizing the speech in the recording\n",
        "    # if a RequestError or UnknownValueError exception is caught,\n",
        "    #     update the response object accordingly\n",
        "    try:\n",
        "        response[\"transcription\"] = recognizer.recognize_google(audio)\n",
        "    except sr.RequestError:\n",
        "        # API was unreachable or unresponsive\n",
        "        response[\"success\"] = False\n",
        "        response[\"error\"] = \"API unavailable\"\n",
        "    except sr.UnknownValueError:\n",
        "        # speech was unintelligible\n",
        "        response[\"error\"] = \"Unable to recognize speech\"\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    recognizer = sr.Recognizer()\n",
        "    microphone = sr.Microphone()\n",
        "\n",
        "    print(\"these are the stopwords i will use \\n\")\n",
        "   # print(stopwords.words('english'))\n",
        "    #words_to_filter = set(stopwords.words('english'))\n",
        "    \n",
        "    \n",
        "\n",
        "    instruction = \"ask me question based on a specific attribute for my character\"\n",
        "    print(instruction)\n",
        "    time.sleep(1)\n",
        "\n",
        "    PROMPT_LIMIT = 1 #number of times a user is allowed to speak to microphone\n",
        "\n",
        "    for i in range(PROMPT_LIMIT):\n",
        "        response_from_user = recognize_speech(recognizer, microphone)\n",
        "        \n",
        "        if not response_from_user[\"success\"]:\n",
        "            break\n",
        "        print(\"I didn't catch that. What did you say?\\n\")\n",
        "\n",
        "    print(\"You said: {}\".format(response_from_user[\"transcription\"]))\n",
        "\n",
        "    # word_tokens = word_tokenize(response_from_user[\"transcription\"])\n",
        "\n",
        "    # filtered_sentence = [w for w in word_tokens if not w.lower() in words_to_filter]\n",
        " \n",
        "    # filtered_sentence = []\n",
        "\n",
        "    # for w in word_tokens:\n",
        "    #     if w not in words_to_filter:\n",
        "    #         filtered_sentence.append(w)\n",
        "\n",
        "    #print(\"after filtering out words we dont need, you said \" + str(filtered_sentence))\n",
        "\n",
        "    print(\"these are all the microphone inputs I can find \" + str(sr.Microphone.list_microphone_names()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndbh8j0x8mHC"
      },
      "source": [
        "## Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "DfNYmqxCgPvS",
        "outputId": "3050d203-8c09-4d11-ab02-e3dd5b439112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">are they anita</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "transcription = \"are they anita\"\n",
        "\n",
        "# Testing the model\n",
        "doc = nlp(transcription)\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "# for ent in doc.ents:\n",
        "# \tprint(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "displacy.render(doc, style='ent',jupyter=True)\n",
        "\n",
        "## ent.text ('blond') and ent.label ( 'hair_color') will then be sent to the game backend to check\n",
        "##\n",
        "## guess_trait = ent.label\n",
        "## guess_adj = ent.text\n",
        "## if guess_trait == guess_adj:\n",
        "##\t\treturn affirmative_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhtNSupMfHoM",
        "outputId": "51972ea9-5ea5-492c-95ce-070d5c6b145e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to /content\n",
            "Loading from /content\n",
            "Entities [('brown', 'glasses')]\n"
          ]
        }
      ],
      "source": [
        "# Save the  model to directory\n",
        "output_dir = Path('/content/')\n",
        "nlp.to_disk(output_dir)\n",
        "print(\"Saved model to\", output_dir)\n",
        "\n",
        "transcription = \"is their hair brown?\" \n",
        "\n",
        "# Load the saved model and predict\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp_updated = spacy.load(output_dir)\n",
        "doc = nlp_updated(transcription)\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "I7mbayPNfO4o",
        "LO4l7xh5gLhj",
        "j_-AOA2ZgOlp"
      ],
      "name": "Copy of Updated_NER_Demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}